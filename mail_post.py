import boto3
from botocore.exceptions import ClientError
import mysql.connector
import gspread
import datetime
import json
import os
import logging
import re
from flask import Flask, request, jsonify

import asyncio
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

import datetime
import time
import logging
import copy

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(filename='/home/ubuntu/job_posting/job_posting.log',
                    level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# í˜„ì¬ ë‚ ì§œ ë¡œê·¸
logging.info("Program started")
logging.info(f"Today's date: {datetime.datetime.today().strftime('%Y-%m-%d')}")

# Selenium ì›¹ ë“œë¼ì´ë²„ ì„¤ì • í•¨ìˆ˜
def setup_driver():
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36")
    driver = webdriver.Chrome(options=options)
    return driver

# ì±„ìš©ê³µê³  db
def connect_to_database():
    db_host = 'localhost'
    db_user = 'root'
    db_password = '1231'
    db_name = 'crawling'
    db_port = 3306
    
    try:
        connection = mysql.connector.connect(host=db_host,
                                             user=db_user,
                                             password=db_password,
                                             db=db_name,
                                             port=db_port,
                                             charset='utf8mb4',
                                             collation='utf8mb4_unicode_ci'
                                             )
        logging.info("Database connection successful")
        return connection
    except mysql.connector.Error as err:
        logging.error(f"Database connection error: {err}")
        return None

# ìˆ˜ì‹ ì ì •ë³´ í…Œì´ë¸”(lms) ë¶ˆëŸ¬ì˜¤ê¸°
def connect_to_lms_database():
    db_host = '*******'  
    db_user = '******'
    db_password = '******!'
    db_name = '*****'
    db_port = ****
    
    try:
        connection = mysql.connector.connect(host=db_host,
                                             user=db_user,
                                             password=db_password,
                                             db=db_name,
                                             port=db_port,
                                             charset='utf8mb4',
                                             collation='utf8mb4_unicode_ci'
                                             )
        logging.info("Database connection successful")
        return connection
    except mysql.connector.Error as err:
        logging.error(f"Database connection error: {err}")
        return None
    
# êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ìˆ˜ì‹ ê±°ë¶€ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
json_file_path = "/home/ubuntu/job_posting/GOOGLE_API/genia_email-recommand-6976a7d469c3.json" 
gc = gspread.service_account(json_file_path) 
spreadsheet_url = "https://docs.google.com/spreadsheets/d/1GdC3sv6q-t2v25alAmS83M76eDsrhfZBwTFOrd0Q1jw/edit?resourcekey=&gid=277815760#gid=277815760"
worksheet = gc.open_by_url(spreadsheet_url)
sheet = worksheet.worksheet("UnsubList")
rows = sheet.get_all_values()

# nê°œì›” í›„ ë‚ ì§œë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
def add_months(date, months):
    # ì •ìˆ˜ ë¶€ë¶„ì€ ì „ì²´ ì›” ìˆ˜, ì†Œìˆ˜ ë¶€ë¶„ì€ ì¼ ìˆ˜ë¡œ ê³„ì‚°
    whole_months = int(months)
    additional_days = int((months - whole_months) * 30) # 0.5ê°œì›”ì„ 15ì¼ë¡¤ ê³„ì‚°
    
    #ì›” ë‹¨ìœ„ ê³„ì‚°
    month = date.month + whole_months
    year = date.year + (month - 1) // 12
    month = (month -1) % 12 + 1
    # day = min(date.day, (datetime.datetime(year, month + 1, 1) - timedelta(days=1)).day)
    try:
    # ì›” ë‹¨ìœ„ ê³„ì‚° ê²°ê³¼
        new_date = datetime.datetime(year, month, date.day)
    except ValueError:
        last_day_of_month = (datetime(year, month +1, 1) - timedelta(days=1)).day
        new_date = datetime(year, month, last_day_of_month)
    
    # ì¶”ê°€ ì¼ ë‹¨ìœ„ ê³„ì‚°
    final_date = new_date + timedelta(days=additional_days)
    return final_date

def insert_and_update_records(insert_months, update_months):
    connection = connect_to_lms_database()
    cursor = connection.cursor(dictionary=True)
    
    try:
        #ì˜¤ëŠ˜ ë‚ ì§œ ê³„ì‚°
        today = datetime.datetime.today().date()
        print(f"ì˜¤ëŠ˜ ë‚ ì§œ: {today}")
      
        cursor.execute("""
                       SELECT *
                       FROM lms_test.course
                       """)
        courses = cursor.fetchall()
        
        for course in courses:
            if course['start_date']:
                # ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜ ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ
                if isinstance(course['start_date'], str):
                    start_date = datetime.datetime.strptime(course['start_date'], '%Y-%m-%d').date()
                else:
                    start_date = course['start_date'].date()
                target_date_insert = add_months(start_date, insert_months)
                
                if target_date_insert.date() <= today:
                    no = course['no']
                    print(f"ì¡°ê±´ì— ë§ëŠ” ì½”ìŠ¤ ë°œê²¬ (ì—…ë°ì´íŠ¸)! no: {no}, target_date_insert: {target_date_insert}")
                    
                    # member í…Œì´ë¸”ì—ì„œ cno ê°’ì´ flag ê°’ì´ ì¼ì¹˜í•˜ëŠ” ë ˆì½”ë“œ ê°’ ê°€ì ¸ì˜¤ê¸°
                    cursor.execute("""
                                   SELECT *
                                   FROM lms_test.member
                                   WHERE cno = %s
                                   """, (no,))
                    members = cursor.fetchall()
                    print(f"ì¼ì¹˜í•˜ëŠ” ë©¤ë²„ ìˆ˜: {len(members)}")

                    # member í…Œì´ë¸”ì˜ ì¼ì¹˜í•˜ëŠ” ë ˆì½”ë“œë§Œ test_member í…Œì´ë¸”ì— ì—¡ë°ì´íŠ¸
                    for member in members:
                        cursor.execute("""
                                   UPDATE lms_test.member
                                   SET send_email = '1'
                                   WHERE cno = %s AND send_email = '0'
                                   """, (no,))
                    connection.commit() 
                    # print(f"Record updated for member: {member['name']} with cno: {member['cno']}")
                    
                    
            if course['end_date']:
                # ë¬¸ìì—´ì„ datetime  ê°ì²´ë¡œ ë³€í™˜
                end_date = datetime.datetime.strptime(course['end_date'], '%Y-%m-%d').date()
                target_date_update = add_months(end_date, update_months) # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ nì¼ í›„
                
                # end_dataì—ì„œ nê°œì›” í›„ì˜ ë‚ ì§œê°€ ì˜¤ëŠ˜ê³¼ ê°™ìœ¼ë©´ ë ˆì½”ë“œ ë³€ê²½
                if target_date_update.date() <= today:
                    # flag = course['flag']
                    print(f"ì¡°ê±´ì— ë§ëŠ” ì½”ìŠ¤ ë°œê²¬! (ì—…ë°ì´íŠ¸)! cno: {course['no']}, target_date_update: {target_date_update}")
                    
                    # member í…Œì´ë¸”ì—ì„œ cnoê°’ì´ no ê°’ê³¼ ì¼ì¹˜í•˜ëŠ” ë ˆì½”ë“œ ê°€ì ¸ì˜¤ê¸°
                    cursor.execute("""
                                   UPDATE lms_test.member
                                   SET send_email = '0'
                                   WHERE cno = %s AND send_email = '1'
                                   """, (course['no'],))
                    connection.commit() 
                    print(f"send_email updated to 0 for cno: {course['no']}")
                        
    except mysql.connector.Error as err:
        print(f"Error: {err}")
    finally:    
        cursor.close()
        connection.close()

# ì‹¤í–‰ 4.5ê°œì›”, 6ê°œì›”
insert_and_update_records(4.5, 6)

# DB ì—°ê²° ë° status ì¡°ê±´ì— ë”°ë¼ send_email ì—…ë°ì´íŠ¸
try:
    db_connection = connect_to_lms_database()
    cursor = db_connection.cursor(dictionary=True)

    # status ì»¬ëŸ¼ì´ 'EMPLOYED', 'OUT', 'REST'ì¸ ì‚¬ìš©ì ê°€ì ¸ì˜¤ê¸°
    cursor.execute("""
        SELECT name, email, status, send_email
        FROM lms_test.member
        WHERE status IN ('EMPLOYED', 'OUT', 'REST')
    """)
    users = cursor.fetchall()  # ì¡°ê±´ì— ë§ëŠ” ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

    for user in users:
        # send_email í•„ë“œê°€ 1ì¼ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
        if user['send_email'] == '1':
            update_query = "UPDATE lms_test.member SET send_email = 0 WHERE name = %s AND email = %s"
            cursor.execute(update_query, (user['name'], user['email']))
            db_connection.commit()
            print(f"{user['name']}ì˜ send_email ê°’ì´ ì„±ê³µì ìœ¼ë¡œ 0ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"{user['name']}ì˜ send_email ê°’ì€ ì´ë¯¸ 0ì…ë‹ˆë‹¤.")

        cursor.execute("""
                    UPDATE lms_test.member
                    SET send_email = '1'
                    WHERE role = 'MANAGER'
                    """)
    db_connection.commit()
    print("roleì´ 'MANAGER'ì¸ ì‚¬ìš©ìë“¤ì˜ send_emailê°’ì´ 1ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

except mysql.connector.Error as e:
    print("DB ì˜¤ë¥˜:", e)

finally:
    cursor.close()
    db_connection.close()

# ê° í–‰ì— ëŒ€í•´ ì´ë¦„ê³¼ ê³¼ì • ì •ë³´ ì¶”ì¶œ ë° ë°ì´í„°ë² ì´ìŠ¤ì™€ ì‹œíŠ¸ ì—…ë°ì´íŠ¸ - ìˆ˜ì‹ ê±°ë¶€
for idx, row in enumerate(rows[1:], start=2):  # ì²« ë²ˆì§¸ í–‰(í—¤ë”) ì œì™¸
    if len(row) > 2 and '/' in row[2]:  # 3ë²ˆì§¸ ì—´ì´ ìˆê³  '/' í¬í•¨ ì‹œì—ë§Œ ì§„í–‰
        name, email = row[2].split('/')[0].strip(), row[2].split('/')[1].strip()
        print(f"ì´ë¦„: {name}, ë©”ì¼: {email}")
        
        # DBì—ì„œ ì´ë¦„ê³¼ ë©”ì¼ì´ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì‚¬
        try:
            db_connection = connect_to_lms_database()
            cursor = db_connection.cursor(dictionary=True)

            # DBì—ì„œ ì´ë¦„ê³¼ ë©”ì¼ì´ ì¼ì¹˜í•˜ëŠ” ì‚¬ìš©ì ê°€ì ¸ì˜¤ê¸°
            cursor.execute("""
                SELECT tm.name, tm.email, tm.send_email, tm.status
                FROM lms_test.member tm
                JOIN lms_test.course tc ON tm.cno = tc.no
                WHERE tm.name = %s AND tm.email = %s
            """, (name, email))
            users = cursor.fetchall()  # ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            
            if users:  # ì´ë¦„ê³¼ ì´ë©”ì¼ì´ ì¼ì¹˜í•˜ëŠ” ì‚¬ìš©ìê°€ ìˆì„ ê²½ìš°        
                for user in users:
                    # ì‚¬ìš©ì send_email í•„ë“œ ì²˜ë¦¬
                    if user['send_email'] == '1':
                        # DBì—ì„œ send_email í•„ë“œ ì—…ë°ì´íŠ¸
                        update_query = "UPDATE lms_test.member SET send_email = 0 WHERE name = %s AND email = %s"
                        cursor.execute(update_query, (user['name'], user['email']))  # ì‚¬ìš©ì ì´ë©”ì¼ë¡œ ì—…ë°ì´íŠ¸
                        db_connection.commit()
                        print(f"{user['name']}ì˜ send_email ê°’ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    elif user['send_email'] == '0':
                        print(f"{user['name']}ì˜ send_email ê°’ì€ ì´ë¯¸ 0ì…ë‹ˆë‹¤.")
            else:
                print(f"{name} ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        except mysql.connector.Error as e:
            print("DB ì˜¤ë¥˜:", e)
        finally:
            cursor.close()
            db_connection.close()

# sql ì¿¼ë¦¬ ì½ê¸°
def get_sql_query_from_file(file_path):
    try:
        with open(file_path, 'r') as file:
            sql_query = file.read()
        logging.info(f"Successfully read SQL query from {file_path}")
        return sql_query
    except Exception as e:
        logging.error(f"Error reading SQL query from file: {e}")
        return None

# test2ì¿¼ë¦¬ ì¡°ê±´ì— ë”°ë¼ dbì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
def get_data_from_database(connection):
    try:
        cursor = connection.cursor()
        sql = get_sql_query_from_file('/home/ubuntu/job_posting/query.sql')
        # logging.info(f"Executing SQL query: {sql}")
        cursor.execute(sql)
        result = cursor.fetchall()
        logging.info(f"Number of rows fetched: {len(result)}")  # ê²°ê³¼ í–‰ ê°œìˆ˜ í™•ì¸
        data = ""
        # ì„œë¡œ ë‹¤ë¥¸ í…Œì´ë¸”ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        for row in result:
            # logging.info(f"Row data: {row}")  # ê° í–‰ ë¡œê·¸ ì¶œë ¥
            data += "{}|{}|{}|{}|{}|".format(row[0], row[1], row[2], row[3], row[4].strftime('%Y-%m-%d %H:%M:%S')) # ì—…ì²´ëª…, ë§ˆê°ë‚ ì§œ, ì œëª©, ê³µê³ url, crawling_time
        logging.info("Successfully fetched data from database")
        return data
    except Exception as e:
        logging.error(f"Error fetching data from database: {e}") 
        return ""

# ì´ë©”ì¼ ì „ì†¡
def send_email(sender_email, recipient_email, subject, html_body):
    aws_region = '**********'
    aws_access_key_id = '*************'
    aws_secret_access_key = '***********'
    
    # ses_client ìƒì„±
    ses_client = boto3.client('ses',
                              region_name = aws_region,
                              aws_access_key_id = aws_access_key_id,
                              aws_secret_access_key = aws_secret_access_key
                              )
    
    # 'chunjae'ë¼ëŠ” ë¬¸ìì—´ì´ recipient_emailì— í¬í•¨ëœ ê²½ìš° ConfigurationSetNameì„ ìƒëµ
    config_set = None  # ê¸°ë³¸ì ìœ¼ë¡œ Configuration Setì„ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì„¤ì •

    if 'chunjae' in recipient_email:  # ì œëª©ì— 'chunjae'ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´
        config_set = None  # ConfigurationSetNameì„ Noneìœ¼ë¡œ ì„¤ì •
    else:
        config_set = 'observe'  # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 'observe'ë¼ëŠ” Configuration Setì„ ì‚¬ìš©
    
    try:
        response = ses_client.send_email(
            Destination = {
                'ToAddresses': [
                    recipient_email
                ],
            },
            Message = {
                'Body' : {
                    'Html' : {
                        'Charset' : 'UTF-8',
                        'Data' : html_body
                    },
                },
                'Subject' : {
                    'Charset' : 'UTF-8',
                    'Data' : subject
                },
            },
            # ConfigurationSetNameì„ ë™ì ìœ¼ë¡œ ì„¤ì •
            **({"ConfigurationSetName": config_set} if config_set else {}),  # config_setì´ Noneì´ë©´ ì œê±°
            Source = sender_email,
        )
        logging.info(f"Email sent successfully to {recipient_email}")
    except ClientError as e:
        logging.error(f"Error sending email: {e.response['Error']['Message']}")
    else:
        logging.info("Email sent successfully")

# ì´ë©”ì¼ ë°œì†¡í•  ëª©ë¡ ì „ì†¡í•˜ëŠ” í•¨ìˆ˜
def read_html_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        html_content = file.read()
    return html_content

# ì§ë¬´ ì¡°ê±´ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def create_job_conditions(course):
    if 'ë¹…ë°ì´í„°' in course or 'BIGDATA' in course:
        return ['ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸', 'ë°ì´í„° ì—”ì§€ë‹ˆì–´', 'ë°ì´í„° ë¶„ì„ê°€', 'ë°ì´í„°ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸', 'ë°ì´í„°ì—”ì§€ë‹ˆì–´', 'ë°ì´í„°ë¶„ì„ê°€',
        'ë°ì´í„° ë¶„ì„','ë°ì´í„°ë¶„ì„', 'ë°ì´í„° ì •ì œ', 'ë°ì´í„°ì •ì œ', 'ë°ì´í„° ì²˜ë¦¬', 'ë°ì´í„°ì²˜ë¦¬', 'ai ê¸°íš', 'AI ê¸°íš', 'aiê¸°íš',
        'ë°ì´í„° ê´€ë¦¬', 'ë°ì´í„° ë¶„ì„ ë§¤ë‹ˆì €', 'ë¨¸ì‹ ëŸ¬ë‹', 'AI ì—”ì§€ë‹ˆì–´', 'ì¸ê³µì§€ëŠ¥ ì—”ì§€ë‹ˆì–´', 'ë°ì´í„° ì‹œê°í™”', 'tableau', 'Tableau'
        'ë°ì´í„° ë§ˆì´ë‹', 'ë¹„ì¦ˆë‹ˆìŠ¤ ì¸í…”ë¦¬ì „ìŠ¤', 'ETL ê°œë°œ', 'SQL', 'R ë¶„ì„', 'Hadoop', 'ì—ë“€í…Œí¬ ì½˜í…ì¸  ê°œë°œ', 'ë”¥ëŸ¬ë‹/ë¨¸ì‹ ëŸ¬ë‹',
        'Data Scientist', 'Data Engineer', 'Data Analyst', 'Machine Learning', 'AI Engineer', 'Data Visualization', 
        'Business Intelligence', 'ETL Developer', 'Big Data Engineer', 'Data Management', 'Data Mining', 'ë”¥ëŸ¬ë‹', 'ìì—°ì–´ ì²˜ë¦¬',
        'Deep Learning Engineer', 'Natural Language Processing', 'NLP', 'DBA', 'ë¹…ë°ì´í„°' 'AI ëª¨ë¸','DBê´€ë¦¬', 'dbê´€ë¦¬',
        'ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸', 'ì¸ê³µì§€ëŠ¥', 'ë°ì´í„°ë¶„ì„','ë°ì´í„°ì²˜ë¦¬','ë°ì´í„°ê´€ë¦¬','ë°ì´í„°ë§ˆì´ë‹', 'LLM','Data Architect','ë°ì´í„° ë¦¬í„°ëŸ¬ì‹œ']
    elif 'í’€ìŠ¤íƒ' in course or 'FULLSTACK' in course:
        return ['í”„ë¡ íŠ¸ì—”ë“œ', 'í”„ë¡ íŠ¸ì•¤ë“œ' 'ë°±ì—”ë“œ', 'ë°±ì•¤ë“œ', 'ì›¹ ì„œë¹„ìŠ¤', 'ëª¨ë°”ì¼ ì„œë¹„ìŠ¤','í’€ìŠ¤íƒ', 'Java', 'ì›¹ ê°œë°œì',
        'ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´', 'ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì', 'ì‹œìŠ¤í…œ ì—”ì§€ë‹ˆì–´', 'ì‹œìŠ¤í…œ ê°œë°œì', 'ëª¨ë°”ì¼ ê°œë°œì',
        'ì•± ê°œë°œì', 'API ê°œë°œì', 'í´ë¼ìš°ë“œ ì—”ì§€ë‹ˆì–´', 'DevOps ì—”ì§€ë‹ˆì–´', 'DevOps ê°œë°œì', 'ì„œë²„ ê°œë°œì',
        'ë„¤íŠ¸ì›Œí¬ ì—”ì§€ë‹ˆì–´', 'ë„¤íŠ¸ì›Œí¬ ê°œë°œì', 'Front-End Developer', 'Front-End', 'Back-End'
        'Back-End Developer', 'Full Stack Developer', 'Software Engineer', 'Mobile Developer', 'Cloud Engineer', 
        'API Developer', 'DevOps Engineer', 'Server Developer', 'Web Designer', 'ì›¹ í¼ë¸”ë¦¬ì…”','Vue.js', 
        'Node.js', 'Frontend Engineer', 'Backend Engineer', 'React Developer', 'Node.js Developer', 'Vue.js Developer', 'CI/CD']
    elif 'PM' in course.upper():
        return['ì›¹ ê¸°íš', 'ì•± ê¸°íš', 'ì„œë¹„ìŠ¤ ê¸°íš', 'ì½˜í…ì¸  ê¸°íš', 'ì „ëµ ê¸°íš', 'í”„ë¡œë•íŠ¸ ë§¤ë‹ˆì €']
        # return ['ì›¹ ì„œë¹„ìŠ¤', 'ì„œë¹„ìŠ¤ ê¸°íš', 'ì„œë¹„ìŠ¤ê¸°íš', 'PM', 'í”„ë¡œë•íŠ¸ ë§¤ë‹ˆì €', 'í”„ë¡œë•íŠ¸ë§¤ë‹ˆì €', 'ì„œë¹„ìŠ¤ ê¸°íšì', 'í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €', 'í”„ë¡œì íŠ¸ë§¤ë‹ˆì €', 'ì„œë¹„ìŠ¤ ìš´ì˜',
        # 'ì„œë¹„ìŠ¤ ë””ìì´ë„ˆ', 'ì„œë¹„ìŠ¤ ê¸°íš ë§¤ë‹ˆì €', 'ì„œë¹„ìŠ¤ ê¸°íš PM', 'ì„œë¹„ìŠ¤ ê¸°íš PL', 'ì„œë¹„ìŠ¤ ê¸°íš ë‹´ë‹¹ì', 'ì½˜í…ì¸  ê¸°íš', 'êµìœ¡ ì½˜í…ì¸  ê¸°íš']
    else:
        return []

# ë¶ˆìš©ì–´ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def create_stopwords(course):
    if 'ë¹…ë°ì´í„°' in course or 'BIGDATA' in course:
        return ['ì„ì‚¬', 'ë°•ì‚¬', 'ì½”ë“œì‡', 'êµìœ¡ìƒ', 'êµ­ë¹„', 'ì²­ë…„ìˆ˜ë‹¹', 'ì œì•½', 'ê±´ì„¤', 'ì—°êµ¬', 'ê¸°ê³„', 'í–‰ì •', 'í˜¸í…”', 'ì²œì¬êµìœ¡', 'ì²œì¬êµê³¼ì„œ', 'SeSAC', 'APM', 'apm', 'edi', 'EDI', 'ë§ˆì¼€í„°', 'ì–‘ì„±']
    elif 'í’€ìŠ¤íƒ' in course or 'FULLSTACK' in course:
        return ['ì„ì‚¬', 'ë°•ì‚¬', 'ì½”ë“œì‡', 'êµìœ¡ìƒ', 'êµ­ë¹„', 'ì²­ë…„ìˆ˜ë‹¹', 'ì œì•½', 'ê±´ì„¤', 'ì—°êµ¬', 'ê¸°ê³„', 'í–‰ì •', 'í˜¸í…”', 'ì²œì¬êµìœ¡', 'ì²œì¬êµê³¼ì„œ', 'SeSAC', 'APM', 'apm', 'edi', 'EDI', 'UX', 'UI', 'ë§ˆì¼€í„°','ì–‘ì„±']
    elif 'PM' in course.upper():
        return ['ì°½ì—…', 'ë²ˆì—­', 'ì„ì‚¬', 'ë°•ì‚¬', 'ì½”ë“œì‡', 'êµìœ¡ìƒ', 'êµ­ë¹„', 'ì²­ë…„ìˆ˜ë‹¹', 'ì œì•½', 'ê±´ì„¤', 'ì—°êµ¬', 'ê¸°ê³„', 'í–‰ì •', 'í˜¸í…”', 'ì²œì¬êµìœ¡', 'ì²œì¬êµê³¼ì„œ', 'SeSAC', 'APM', 'apm', 'edi', 'EDI', 'ê°œë°œì', 'ì‹œê³µ','ìœ íŠœë¸Œ','ì–‘ì„±']
    else:
        return []

def get_unsub_data(connection):
    try:
        cursor = connection.cursor(dictionary=True)
        logging.info("Cursor initialized.")
        sql = """
        SELECT tm.name, tm.email, tm.role, tm.course_nos, tm.cno, tm.send_email
        FROM lms_test.member tm
        WHERE tm.send_email <> '0' AND tm.send_email <> '2'
        """
        cursor.execute(sql)
        members = cursor.fetchall() #ëª¨ë“  í–‰ ê°€ì ¸ì˜¤ê¸°
        logging.info(f"Members fetched: {members}")
        
        unsubscribed_data =[]
        
        for member in members:
            processed_subject = set() #ì´ë¯¸ ì²˜ë¦¬í•œ subjectë¥¼ ì¶”ì 
            logging.info(f"Processing member: {member}")
            if member['role'] == 'MANAGER' and member['course_nos']:
                # course_nos ì²˜ë¦¬
                course_ids = [int(course_id.strip()) for course_id in member['course_nos'].split(',') if course_id.strip().isdigit()]
                if not course_ids:
                    logging.info(f'No course IDs for memeber: {member['name']}')
                    continue
                
                #course_idsë¥¼ ì‚¬ìš©í•˜ì—¬ ë§¤ì¹­ë˜ëŠ” ì½”ìŠ¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜´
                sql_course="""
                SELECT no,subject
                FROM lms_test.course
                WHERE no IN (%s)
                """ % ','.join(['%s'] * len(course_ids)) # IN ì ˆ ë™ì ìœ¼ë¡œ êµ¬ì„±
                cursor.execute(sql_course, course_ids)
                courses = cursor.fetchall()
                logging.info(f'Courses fetched for member:{courses}')
                
                for course in courses:
                    if course['subject'] not in processed_subject:
                        unsubscribed_data.append({
                            'name' : member['name'],
                            'email' : member['email'],
                            'subject' : course['subject'],
                            'send_email' : member['send_email']
                        })
                        processed_subject.add(course['subject']) # ì¤‘ë³µ ë°©ì§€ 
            elif member['cno']:
                # MANAGERê°€ ì•„ë‹Œ ê²½ìš° ê¸°ì¡´ cnoë¥¼ ì‚¬ìš©í•˜ì—¬ ì½”ìŠ¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜´
                sql_course="""
                SELECT no,subject
                FROM lms_test.course
                WHERE no=%s
                """
                cursor.execute(sql_course, (member['cno'],))
                course = cursor.fetchone()
                logging.info(f"Single course fetched:{course}")
                if course:
                    unsubscribed_data.append({
                        'name': member['name'],
                        'email': member['email'],
                        'subject': course['subject'],
                        'send_email': member['send_email']
                    })
                    processed_subject.add(course['subject']) # ë©¤ë²„ì˜ ì²˜ë¦¬ëœ subject ì¶”ê°€
        logging.info(f"Unsubscribed Data: {unsubscribed_data}")
        return unsubscribed_data
    
    except mysql.connector.Error as err:
        logging.error(f"Error fetching data: {err}")
        return None

async def fetch_job_details(job_key, source, job_url, title, stop_words, to_delete):
    driver = setup_driver()
    try:
        driver.get(job_url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*')))
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')

        if "incruit" in job_url:
            text = driver.find_element(By.XPATH, '//*[@id="headInfoRight"]/div[2]/button').text
        elif "jumpit" in job_url:
            # text = soup.find('div', class_='sc-190507ae-1 bAkpEB').text
            text = soup.find('div', class_='sc-407d4306-1 mirSB').text
        elif "wanted" in job_url:
            text = soup.find('div', class_='WantedApplyBtn_container__fjWVh').text
        elif "saramin" in job_url:
            text = driver.find_element(By.XPATH, '//*[@id="content"]/div[3]/section[1]/div[1]/div[1]/div/div[2]/button/span').text

        time.sleep(1)
        logging.info(f'Text from {source} - {title}: {text}')

        # í…ìŠ¤íŠ¸ì— íŠ¹ì • ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if any(word in text for word in stop_words):
            # ì‚­ì œí•  í•­ëª© ì €ì¥
            to_delete.append(job_key)  # job_keyë¥¼ to_deleteì— ì¶”ê°€

    except Exception as e:
        logging.error(f"Error with {job_url}: {e}")
        to_delete.append(job_key)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì‚­ì œ í•­ëª©ì— ì¶”ê°€
    finally:
        driver.quit()

async def process_source(source, postings, stop_words):
    to_delete = []  # ì´ ë¦¬ìŠ¤íŠ¸ëŠ” ì‚­ì œí•  ì‘ì—… í‚¤ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    tasks = []

    for company, duedate, title, job_url, crawling_time in postings:
        company_cleaned = re.sub(r"\(ì£¼\)|ì£¼ì‹íšŒì‚¬\s*|\s*ì£¼ì‹íšŒì‚¬\s*", "", company).strip()
        job_key = (company_cleaned, title)

        task = asyncio.create_task(fetch_job_details(job_key, source, job_url, title, stop_words, to_delete))
        tasks.append(task)

        # 4ê°œ ì‘ì—…ì„ ì§„í–‰í•œ í›„ ëŒ€ê¸°
        if len(tasks) == 4:
            await asyncio.gather(*tasks)
            tasks = []  # ì‘ì—… ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”

    # ë‚¨ì•„ìˆëŠ” ì‘ì—… ì‹¤í–‰
    if tasks:
        await asyncio.gather(*tasks)

    return to_delete  # ì‚­ì œí•  í•­ëª© ëª©ë¡ì„ ë°˜í™˜

async def main(grouped_data, stop_words, unique_jobs):
    all_to_delete = []
    tasks = []

    # ê° sourceì— ëŒ€í•´ ë¹„ë™ê¸° íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
    for source, postings in grouped_data.items():
        print(f"Processing source: {source}")
        initial_count = len(postings)  # ì´ˆê¸° ë°ì´í„° ê°œìˆ˜ ê¸°ë¡
        
        limited_postings = postings[:50]# í¬ìŠ¤íŒ…ì„ ìµœëŒ€ 10ê°œë¡œ ì œí•œ
        task = asyncio.create_task(process_source(source, limited_postings, stop_words))
        tasks.append((source, initial_count, task))  # ì†ŒìŠ¤ì™€ ì´ˆê¸° ê°œìˆ˜ í•¨ê»˜ ì €ì¥

    # ëª¨ë“  íƒœìŠ¤í¬ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
    results = await asyncio.gather(*[task for _, _, task in tasks])

    # ê° sourceì˜ to_delete ë¦¬ìŠ¤íŠ¸ë¥¼ all_to_deleteì— ì¶”ê°€
    for index in range(len(tasks)):
        source, initial_count, _ = tasks[index]  # ê° sourceì™€ ì´ˆê¸° ê°œìˆ˜ ê°€ì ¸ì˜¤ê¸°
        to_delete = results[index]  # ê²°ê³¼ì—ì„œ to_deleteë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        # to_deleteì— course ì •ë³´ë¥¼ ì¶”ê°€í•˜ì—¬ ìµœì¢… ì‚­ì œí•  ë¦¬ìŠ¤íŠ¸ ìƒì„±
        for job_key in to_delete:
            company, title = job_key
            # ê° courseë³„ë¡œ to_deleteë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
            for course in unique_jobs.keys():
                if job_key in unique_jobs[course]:
                    all_to_delete.append((course, company, title))

        final_count = initial_count - len(to_delete)  # ì‚­ì œ í›„ ë‚¨ì€ ë°ì´í„° ê°œìˆ˜
        print(f"Source: {source} - Initial count: {initial_count}, Deleted: {len(to_delete)}, Final count: {final_count}")

    return all_to_delete  # ìµœì¢… ì‚­ì œí•  ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

course_list = ['BIGDATA', 'FULLSTACK', 'PM']

def fetch_and_deduplicate_data(course_list):
    unique_jobs = {course: {} for course in course_list}
    
    for course in course_list:
        print(f"Fetching data for course: {course}")
        job_conditions = create_job_conditions(course)
        stopwords_list = create_stopwords(course)
        with open('/home/ubuntu/job_posting/query_base.sql', 'r', encoding='utf-8') as file:
            query = file.read()
            job_conditions_sql_1 = ' OR '.join([f'(saramin_data.job_title LIKE "%{job}%")' for job in job_conditions])
            job_conditions_sql_2 = ' OR '.join([f'(incruit_data.job_title LIKE "%{job}%")' for job in job_conditions])
            job_conditions_sql_3 = ' OR '.join([f'(new_wanted_data.job_title LIKE "%{job}%")' for job in job_conditions])
            job_conditions_sql_4 = ' OR '.join([f'(jumpit_data.job_title LIKE "%{job}%")' for job in job_conditions])
            # ë¶ˆìš©ì–´ ì¡°ê±´ ì¶”ê°€ - ê³µê³ ëª…
            stopword_sql_1 = 'AND'.join([f'(saramin_data.job_title NOT LIKE "%{word}%")' for word in stopwords_list])
            stopword_sql_2 = 'AND'.join([f'(incruit_data.job_title NOT LIKE "%{word}%")' for word in stopwords_list])
            stopword_sql_3 = 'AND'.join([f'(new_wanted_data.job_title NOT LIKE "%{word}%")' for word in stopwords_list])
            stopword_sql_4 = 'AND'.join([f'(jumpit_data.job_title NOT LIKE "%{word}%")' for word in stopwords_list])

            # ë¶ˆìš©ì–´ ì¡°ê±´ ì¶”ê°€ - íšŒì‚¬ëª…
            stopword_sql_5 = 'AND'.join([f'(saramin_data.company_name NOT LIKE "%{word}%")' for word in stopwords_list])
            stopword_sql_6 = 'AND'.join([f'(incruit_data.company NOT LIKE "%{word}%")' for word in stopwords_list])
            stopword_sql_7 = 'AND'.join([f'(new_wanted_data.company_name NOT LIKE "%{word}%")' for word in stopwords_list])
            stopword_sql_8 = 'AND'.join([f'(jumpit_data.company_name NOT LIKE "%{word}%")' for word in stopwords_list])

        # ì¿¼ë¦¬ ìƒì„± ë° ì €ì¥   
        query = query.format(
                condition1=f"({job_conditions_sql_1}) AND ({stopword_sql_1}) AND ({stopword_sql_5})",
                condition2=f"({job_conditions_sql_2}) AND ({stopword_sql_2}) AND ({stopword_sql_6})",
                condition3=f"({job_conditions_sql_3}) AND ({stopword_sql_3}) AND ({stopword_sql_7})",
                condition4=f"({job_conditions_sql_4}) AND ({stopword_sql_4}) AND ({stopword_sql_8})"
            )

        with open('/home/ubuntu/job_posting/query.sql', 'w') as output_file:
            output_file.write(query) 

        db_connection = connect_to_database() 
        db_to_html = get_data_from_database(db_connection) 
        logging.info("========================================")
        logging.info(f"Course '{course}' data cached. Number of records: {len(db_to_html)}")
        data = db_to_html.split('|')[:-1]
        # logging.info(data)
        grouped_data = {
            'ì‚¬ëŒì¸' : [],
            'ì¸í¬ë£¨íŠ¸' : [],
            'ì›í‹°ë“œ' : [],
            'ì í•' : []
        }
        # ê° ì†ŒìŠ¤ë³„ë¡œ ë°ì´í„°ë¥¼ ë¶„ë¦¬
        for i in range(0, len(data), 5):
            company, duedate, title, job_url, crawling_time = data[i:i+5]
            crawling_time = datetime.datetime.strptime(crawling_time, '%Y-%m-%d %H:%M:%S')

            if 'jumpit' in job_url:
                grouped_data['ì í•'].append((company, duedate, title, job_url, crawling_time))
            elif 'saramin' in job_url:
                grouped_data['ì‚¬ëŒì¸'].append((company, duedate, title, job_url, crawling_time))
            elif 'incruit' in job_url:
                grouped_data['ì¸í¬ë£¨íŠ¸'].append((company, duedate, title, job_url, crawling_time))
            elif 'wanted' in job_url:
                grouped_data['ì›í‹°ë“œ'].append((company, duedate, title, job_url, crawling_time))
            # crawling_time ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            for source in grouped_data.keys():
                grouped_data[source].sort(key=lambda x: x[4], reverse=True)  # x[4] = crawling_time
                # logging.info(f"{source} ë°ì´í„°: {grouped_data[source]}")

        # ì†ŒìŠ¤ ë‚´ ì¤‘ë³µì„ ë¨¼ì € ì œê±°í•œ í›„, ì†ŒìŠ¤ ê°„ ì¤‘ë³µë„ ì²˜ë¦¬í•˜ì—¬ ìµœì¢… ë°ì´í„°ë¥¼ ì €ì¥
        for source, postings in grouped_data.items():
            
            for company, duedate, title, job_url, crawling_time in postings:
                company_cleaned = re.sub(r"\(ì£¼\)|ì£¼ì‹íšŒì‚¬\s*|\s*ì£¼ì‹íšŒì‚¬\s*", "", company).strip()
                job_key = (company_cleaned, title)
                # ì†ŒìŠ¤ ê°„ ì¤‘ë³µ í™•ì¸: company_cleaned ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì²˜ë¦¬
                if job_key not in unique_jobs[course]:  # ì½”ìŠ¤ë³„ë¡œ í™•ì¸
                    unique_jobs[course][job_key] = (source, company, duedate, title, job_url, crawling_time)
         
        # logging.info(f'unique_jobs : {unique_jobs}')  
        # ê° URLì— ëŒ€í•´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        stop_words = ['ë§ˆê°ëœ ê³µê³ ', 'ë§ˆê°ëœ í¬ì§€ì…˜', 'ì§€ì›ë§ˆê°', 'ì ‘ìˆ˜ë§ˆê°']

        start_time = datetime.datetime.now()
        # ë¹„ë™ê¸° ì‹¤í–‰
        logging.info(f'start time : {start_time}')

        # main í•¨ìˆ˜ í˜¸ì¶œë¡œ grouped_dataì™€ stop_words, unique_jobs ì²˜ë¦¬
        to_delete = asyncio.run(main(grouped_data, stop_words, unique_jobs))
        
        # to_deleteì— ìˆëŠ” í•­ëª©ì„ courseë³„ë¡œ unique_jobsì—ì„œ ì œê±°
        for course, company, title in to_delete:
            job_key = (company, title)  # (company, title) í˜•ì‹ì˜ job_key ìƒì„±

            # í•´ë‹¹ courseì—ì„œ job_keyê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ í›„ ì‚­ì œ
            if job_key in unique_jobs.get(course, {}):
                del unique_jobs[course][job_key]  # ì‚­ì œ

        logging.info(f"Items to delete: {to_delete}")

        end_time = datetime.datetime.now()
        finish = end_time - start_time
        logging.info(f'finish time : {finish}')
        # logging.info(unique_jobs)
        logging.info('========================================')

    return unique_jobs, grouped_data

def select_posting(unique_jobs, course):
    fin_course_list = {
        'BIGDATA': {},
        'FULLSTACK': {},
        'PM': {}
    }

    # BIGDATA ë° FULLSTACK ì¡°ê±´ ì²˜ë¦¬
    if 'ë¹…ë°ì´í„°' in course or 'BIGDATA' in course:
        total_bigdata_count = 0
        # ê³µê³ ë¥¼ ì‚¬ì´íŠ¸ë³„ë¡œ ì €ì¥í•  ì„ì‹œ êµ¬ì¡° ìƒì„±
        bigdata_site_jobs = {}
        for (company, job_title), (job_source, company, duedate, title, job_url, crawling_time) in unique_jobs[course].items():
            if job_source not in bigdata_site_jobs:
                bigdata_site_jobs[job_source] = []
            bigdata_site_jobs[job_source].append((company, duedate, title, job_url, crawling_time))
            # print(f"bigdata_site_jobs: {len(bigdata_site_jobs)}")

        # ë¶„ë°°ìš© ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        for job_source in bigdata_site_jobs:
            if job_source not in fin_course_list['BIGDATA']:
                fin_course_list['BIGDATA'][job_source] = []

        # ëª¨ë“  ì‚¬ì´íŠ¸ì—ì„œ ìµœëŒ€ 5ê°œì”© ê³µê³ ë¥¼ ì¶”ê°€
        max_jobs = 5
        while total_bigdata_count < 20:
            any_added = False  # ê° ë¼ìš´ë“œì—ì„œ ê³µê³ ë¥¼ ì¶”ê°€í–ˆëŠ”ì§€ í™•ì¸
            for job_source, jobs in bigdata_site_jobs.items():
                if total_bigdata_count >= 20:
                    break
                if jobs and len(fin_course_list['BIGDATA'][job_source]) < max_jobs:  # ì‚¬ì´íŠ¸ë³„ ìµœëŒ€ ìˆ˜ ì œí•œ
                    fin_course_list['BIGDATA'][job_source].append(jobs.pop(0))  # í•œ ê°œì”© ì¶”ê°€
                    total_bigdata_count += 1
                    any_added = True
                    print(f"[ì¶”ê°€ë¨] ì‚¬ì´íŠ¸: {job_source}, ì´ ê³µê³  ìˆ˜: {total_bigdata_count}")  # ë””ë²„ê¹… ì¶œë ¥
            if not any_added:
                print("ë” ì´ìƒ ì¶”ê°€í•  ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")  # ë””ë²„ê¹… ì¶œë ¥
                break
        # ê³µê³ ê°€ 20ê°œê°€ ì•ˆë˜ë©´ 'ì‚¬ëŒì¸'ì—ì„œ ì¶”ê°€ë¡œ ê³µê³ ë¥¼ ì±„ì›€
        if total_bigdata_count < 20 and 'ì‚¬ëŒì¸' in bigdata_site_jobs:
            while total_bigdata_count < 20 and bigdata_site_jobs['ì‚¬ëŒì¸']:
                fin_course_list['BIGDATA']['ì‚¬ëŒì¸'].append(bigdata_site_jobs['ì‚¬ëŒì¸'].pop(0))
                total_bigdata_count += 1
                print(f"[ì‚¬ëŒì¸ ì¶”ê°€ë¨] ì´ ê³µê³  ìˆ˜: {total_bigdata_count}")  # ë””ë²„ê¹… ì¶œë ¥

    if 'í’€ìŠ¤íƒ' in course or 'FULLSTACK' in course:
        total_fullstack_count = 0
        # ê³µê³ ë¥¼ ì‚¬ì´íŠ¸ë³„ë¡œ ì €ì¥í•  ì„ì‹œ êµ¬ì¡° ìƒì„±
        full_site_jobs = {}
        for (company, job_title), (job_source, company, duedate, title, job_url, crawling_time) in unique_jobs[course].items():
            if job_source not in full_site_jobs:
                full_site_jobs[job_source] = []
            full_site_jobs[job_source].append((company, duedate, title, job_url, crawling_time))
            # print(f"full_site_jobs: {len(full_site_jobs)}")

        # ë¶„ë°°ìš© ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        for job_source in full_site_jobs:
            if job_source not in fin_course_list['FULLSTACK']:
                fin_course_list['FULLSTACK'][job_source] = []
        
        # ëª¨ë“  ì‚¬ì´íŠ¸ì—ì„œ ìµœëŒ€ 5ê°œì”© ê³µê³ ë¥¼ ì¶”ê°€
        max_jobs = 5  
        while total_fullstack_count < 20:
            any_added = False  # ê° ë¼ìš´ë“œì—ì„œ ê³µê³ ë¥¼ ì¶”ê°€í–ˆëŠ”ì§€ í™•ì¸
            for job_source, jobs in full_site_jobs.items():
                if total_fullstack_count >= 20:
                    break
                if jobs and len(fin_course_list['FULLSTACK'][job_source]) < max_jobs:  # ì‚¬ì´íŠ¸ë³„ ìµœëŒ€ ìˆ˜ ì œí•œ
                    fin_course_list['FULLSTACK'][job_source].append(jobs.pop(0))  # í•œ ê°œì”© ì¶”ê°€
                    total_fullstack_count += 1
                    any_added = True
                    print(f"[ì¶”ê°€ë¨] ì‚¬ì´íŠ¸: {job_source}, ì´ ê³µê³  ìˆ˜: {total_fullstack_count}")  # ë””ë²„ê¹… ì¶œë ¥
            if not any_added:
                print("ë” ì´ìƒ ì¶”ê°€í•  ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")  # ë””ë²„ê¹… ì¶œë ¥
                break
        # ê³µê³ ê°€ 20ê°œê°€ ì•ˆë˜ë©´ 'ì‚¬ëŒì¸'ì—ì„œ ì¶”ê°€ë¡œ ê³µê³ ë¥¼ ì±„ì›€
        if total_fullstack_count < 20 and 'ì‚¬ëŒì¸' in full_site_jobs:
            while total_fullstack_count < 20 and full_site_jobs['ì‚¬ëŒì¸']:
                fin_course_list['FULLSTACK']['ì‚¬ëŒì¸'].append(full_site_jobs['ì‚¬ëŒì¸'].pop(0))
                total_fullstack_count += 1
                print(f"[ì‚¬ëŒì¸ ì¶”ê°€ë¨] ì´ ê³µê³  ìˆ˜: {total_fullstack_count}")  # ë””ë²„ê¹… ì¶œë ¥

    # PM ì¡°ê±´ ì²˜ë¦¬
    if 'PM' in course.upper():
        total_pm_count = 0
        # ê³µê³ ë¥¼ ì‚¬ì´íŠ¸ë³„ë¡œ ì €ì¥í•  ì„ì‹œ êµ¬ì¡° ìƒì„±
        pm_site_jobs = {}
        for (company, job_title), (job_source, company, duedate, title, job_url, crawling_time) in unique_jobs[course].items():
            if job_source not in pm_site_jobs:
                pm_site_jobs[job_source] = []
            pm_site_jobs[job_source].append((company, duedate, title, job_url, crawling_time))
            # print(f"pm_site_jobs: {len(pm_site_jobs)}")
        
        # ë¶„ë°°ìš© ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        for job_source in pm_site_jobs:
            if job_source not in fin_course_list['PM']:
                fin_course_list['PM'][job_source] = []
        
        # ëª¨ë“  ì‚¬ì´íŠ¸ì—ì„œ ìµœëŒ€ 5ê°œì”© ê³µê³ ë¥¼ ì¶”ê°€
        max_jobs = 5
        while total_pm_count < 20:
            any_added = False  # ê° ë¼ìš´ë“œì—ì„œ ê³µê³ ë¥¼ ì¶”ê°€í–ˆëŠ”ì§€ í™•ì¸
            for job_source, jobs in pm_site_jobs.items():
                if total_pm_count >= 20:
                    break
                if jobs and len(fin_course_list['PM'][job_source]) < 5:  # ì‚¬ì´íŠ¸ë³„ ìµœëŒ€ 5ê°œ ì œí•œ
                    fin_course_list['PM'][job_source].append(jobs.pop(0))  # í•œ ê°œì”© ì¶”ê°€
                    total_pm_count += 1
                    any_added = True
                    print(f"[ì¶”ê°€ë¨] ì‚¬ì´íŠ¸: {job_source}, ì´ ê³µê³  ìˆ˜: {total_pm_count}")  # ë””ë²„ê¹… ì¶œë ¥
            if not any_added:
                print("ë” ì´ìƒ ì¶”ê°€í•  ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")  # ë””ë²„ê¹… ì¶œë ¥
                break
        # ê³µê³ ê°€ 20ê°œê°€ ì•ˆë˜ë©´ 'ì‚¬ëŒì¸'ì—ì„œ ì¶”ê°€ë¡œ ê³µê³ ë¥¼ ì±„ì›€
        if total_pm_count < 20 and 'ì‚¬ëŒì¸' in pm_site_jobs:
            while total_pm_count < 20 and pm_site_jobs['ì‚¬ëŒì¸']:
                fin_course_list['PM']['ì‚¬ëŒì¸'].append(pm_site_jobs['ì‚¬ëŒì¸'].pop(0))
                total_pm_count += 1
                print(f"[ì‚¬ëŒì¸ ì¶”ê°€ë¨] ì´ ê³µê³  ìˆ˜: {total_pm_count}")  # ë””ë²„ê¹… ì¶œë ¥
    # logging.info(f"ìµœì¢… ë°˜í™˜ ê°’: {fin_course_list}")
    return fin_course_list

# def load_json_jobs(post_json_path):
#     """jsoníŒŒì¼ì˜ ë¦¬ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°"""
#     json_jobs = {}
#     if os.path.exists(post_json_path):
#         with open(post_json_path, 'r', encoding='utf-8') as json_file:
#             json_data = json.load(json_file)
#             # logging.info(f'Loaded JSON data: {json_data}')
#         # ê° í‚¤ì— ì ‘ê·¼í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘
#         categories = ['BIGDATA', 'FULLSTACK', 'PM']
#         for category in categories:
#             category_data = json_data[0]['data'].get(category, {})
#             # logging.info(f'{category} Data: {category_data}')

#         for category in categories:
#             json_jobs[category] = json_data[0]['data'].get(category, {})
#         # logging.info(f'Combined JSON jobs data: {json_jobs}')
#     return json_jobs

def load_json_jobs(post_json_path):
    """jsoníŒŒì¼ì˜ ë¦¬ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°"""
    json_jobs = {}
    if os.path.exists(post_json_path):
        with open(post_json_path, 'r', encoding='utf-8') as json_file:
            json_data = json.load(json_file)
            # logging.info(f'Loaded JSON data: {json_data}')
        # ê° í‚¤ì— ì ‘ê·¼í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘
        categories = ['BIGDATA', 'FULLSTACK', 'PM']

        # ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹˜ëŠ” ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
        combined_data = {category: {} for category in categories}
        # JSONì˜ ëª¨ë“  timestamp ë°ì´í„°ë¥¼ ìˆœíšŒ
        for entry in json_data:
            for category in categories:
                category_data = entry['data'].get(category, {})
                for source, jobs in category_data.items():
                    if source not in combined_data[category]:
                        combined_data[category][source] = []
                    
                    # ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ë°ì´í„°ë§Œ ì¶”ê°€
                    for job in jobs:
                        if job not in combined_data[category][source]:
                            combined_data[category][source].append(job)
        
        json_jobs = combined_data
        # logging.info(f'Combined JSON jobs data: {json_jobs}')
    return json_jobs

def compare_unique_jobs_with_json(json_jobs, unique_jobs):
    """jsonê³¼ ì¤‘ë³µëœ ê³µê³  í™•ì¸ ë° ì œê±°"""
    filtered_jobs = {}

    for category, jobs in unique_jobs.items():
        filtered_jobs[category] = {}
        # logging.info(f"filtered_jobs:{filtered_jobs}")
        for job_key, job_data in jobs.items():
            # `unique_jobs` ë°ì´í„° í˜•ì‹ (source, company, duedate, title, job_url, crawling_time)
            source, company, duedate, title, job_url, _ = job_data
            
            # JSON ë°ì´í„°ì— í•´ë‹¹ ê³µê³ ê°€ ìˆëŠ”ì§€ í™•ì¸
            if category in json_jobs:
                json_category_data = json_jobs[category]

                # JSON ë°ì´í„°ì—ì„œ (company, title) ìŒì„ ì‚¬ìš©í•´ ì¤‘ë³µ í™•ì¸
                json_job_tuples = {}
                for company_name, company_jobs in json_category_data.items():
                    for data in company_jobs:
                        # ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ì§€ í™•ì¸í•˜ê³  ê¸¸ì´ë¥¼ ì²´í¬í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì ‘ê·¼
                        if isinstance(data, list) and len(data) > 2:
                            json_job_tuples[(data[0], data[2])] = data
                
                # í˜„ì¬ ê³µê³ ì˜ í‚¤ë¥¼ (company, title) íŠœí”Œë¡œ ìƒì„±
                job_tuple = (company, title)
                
                # JSON ë°ì´í„°ì™€ ì¤‘ë³µë˜ëŠ” ê²½ìš° ì œê±° ë° ë¡œê·¸
                if job_tuple in json_job_tuples:
                    logging.info(f"ì¤‘ë³µëœ ê³µê³  ì œê±°ë¨: category={category}, source={source}, company={company}, title={title}, duedate={duedate}, job_url={job_url}")
                else:
                    # ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê³µê³ ë§Œ filtered_jobsì— ì¶”ê°€
                    filtered_jobs[category][job_key] = job_data
            else:
                # í•´ë‹¹ categoryê°€ json_jobsì— ì—†ìœ¼ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼í•˜ì§€ ì•ŠìŒ
                filtered_jobs[category][job_key] = job_data

    logging.info(f"ì¤‘ë³µë˜ì§€ ì•Šì€ unique_jobs ìˆ˜: {sum(len(jobs) for jobs in filtered_jobs.values())}")
    return filtered_jobs

def send_emails(unique_jobs, unique_data_list, post_json_path):
    html_header = read_html_file('/home/ubuntu/job_posting/header.html')
    html_tr = read_html_file('/home/ubuntu/job_posting/tr.html')

    # JSON íŒŒì¼ë¡œë¶€í„° ì¤‘ë³µ ê³µê³  ë¶ˆëŸ¬ì˜¤ê¸°
    json_jobs = load_json_jobs(post_json_path)
    # ì¤‘ë³µ ê³µê³ ë¥¼ ì œê±°í•˜ê³  unique_jobs ì—…ë°ì´íŠ¸
    unique_jobs = compare_unique_jobs_with_json(json_jobs, unique_jobs)
    # logging.info(f'unique_jobs{unique_jobs}')
    # ëª¨ë“  ê³¼ì •ì— ëŒ€í•œ ìµœì¢… ë¦¬ìŠ¤íŠ¸ ìƒì„±
    fin_course_list = {}
    # ê° ê³¼ì •ì— ëŒ€í•´ select_posting í˜¸ì¶œí•˜ì—¬ ê²°ê³¼ë¥¼ ì§ì ‘ fin_course_listì— í• ë‹¹
    for course in ['BIGDATA', 'FULLSTACK', 'PM']:
        fin_course_list[course] = select_posting(unique_jobs, course)[course]

    # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    for course, postings in fin_course_list.items():
        for source, job_list in postings.items():
            for i, job in enumerate(job_list):
                company, duedate, title, job_url, crawling_time = job
                job_list[i] = (company, duedate, title, job_url, crawling_time.strftime("%Y-%m-%d %H:%M:%S"))

    # logging.info(f'fin_course_list: {fin_course_list}')

    # JSON íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if os.path.exists(post_json_path):
        # ê¸°ì¡´ ë°ì´í„°ë¥¼ ì½ê³  ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì¶”ê°€
        with open(post_json_path, 'r', encoding='utf-8') as json_file:
            try:
                # ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¡œë“œ
                existing_data = json.load(json_file)
                if not isinstance(existing_data, list):
                    existing_data = []
            except json.JSONDecodeError:
                existing_data = []
                # íŒŒì¼ì´ ë¹„ì–´ ìˆëŠ” ê²½ìš° ì´ˆê¸°í™”
    else:
        # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        existing_data = []
    
    # í˜„ì¬ timestamp ì¶”ê°€
    new_entry = {
        "timestamp": datetime.datetime.now().isoformat(),
        "data": fin_course_list
    }
    # ìƒˆ ë°ì´í„°ë¥¼ ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    existing_data.append(new_entry)

    # ì—…ë°ì´íŠ¸ëœ ì „ì²´ ë°ì´í„°ë¥¼ íŒŒì¼ì— ë‹¤ì‹œ ì €ì¥
    with open(post_json_path, 'w', encoding='utf-8') as json_file:
        json.dump(existing_data, json_file, ensure_ascii=False, indent=2)

    # logging.info(f"ìµœì¢… ì„ íƒ ë¦¬ìŠ¤íŠ¸ê°€ JSON íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {fin_course_list}")
    logging.info(f"=========================ìµœì¢… ì„ íƒ ë¦¬ìŠ¤íŠ¸ê°€ JSON íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤=========================")

    for data in unique_data_list:
        name = data['name']
        email = data['email']
        course = data['subject']
        logging.info(f"Sending email to: {name}, Email: {email}, Course: {course}")
        tr = ""     
        total_job_postings = 0

        # courseì— ëŒ€í•œ ì •ë³´ê°€ fin_course_listì— ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì¶”ê°€
        if course in fin_course_list:  # íŠ¹ì • ê³¼ì •ì´ fin_course_listì— ìˆëŠ”ì§€ í™•ì¸
            postings = fin_course_list[course]  # í•´ë‹¹ ê³¼ì •ì˜ ëª¨ë“  í¬ìŠ¤íŒ… ê°€ì ¸ì˜¤ê¸°
            for source, posting_list in postings.items():  # ê° ì†ŒìŠ¤ë³„ í¬ìŠ¤íŒ… ê°€ì ¸ì˜¤ê¸°
                if posting_list:  # postingsê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                    # ì†ŒìŠ¤ ì´ë¦„ì„ ì¶”ê°€ (ë°‘ì¤„ ì¶”ê°€)
                    tr += f"<h3 style='font-family: Malgun Gothic, sans-serif; font-size: 25px; color: #000000; text-align: center; background-color: #e3eaff; solid;'>{source}</h3>"
                    tr += "<table width='100%' border='0' cellspacing='0' cellpadding='0'>"

                    for job in posting_list:
                        company, duedate, title, job_url, crawling_time = job
                        job_html = html_tr.format(job_url, company, duedate, title)
                        tr += job_html
                        total_job_postings += 1
                        # logging.info(f"Added {course} job: {title} from {company}")

                    tr += "</table>"
        logging.info(f"Total job postings added for {name}: {total_job_postings}")

        today = datetime.datetime.today().strftime("%Y-%m-%d")
                
        html_feed_back = read_html_file('/home/ubuntu/job_posting/feedback.html')
        #ê³µê³  ìˆ˜ê°€ 20ê°œ ë¯¸ë§Œì¼ ê²½ìš° ê²½ê³  ë¬¸êµ¬ ì¶”ê°€
        if total_job_postings < 20:
            tr +="""
            <div style='font-family: Malgun Gothic, sans-serif; font-size:12px; color: #DF3535; text-align: center; margin-top: 20px;'>
            ìµœì‹  ê³µê³  ì œê³µìœ¼ë¡œ ì±„ìš©ê³µê³ ê°€ 20ê°œ ë¯¸ë§Œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            </div>
            """
            
        # í”¼ë“œë°± ë§í¬ ì¶”ê°€
        if html_feed_back: 
            tr +=f"""
            <div style='margin-top: 20px;'>
            {html_feed_back}
            </div>
            """
        # ìˆ˜ì‹  ê±°ë¶€ ì¶”ê°€
        tr +="""
        <div style='font-family: Malgun Gothic, sans-serif; color: #9D9D9D; text-decoration: underline; font-size: 12px; text-align: center;'>
        <a href="https://forms.gle/C785dS1va78w6PMH7">ìˆ˜ì‹  ê±°ë¶€</a>
        </div>
        """
        html_res = html_header.format(today, name, total_job_postings, tr)

        sender_email = 'chunjaecloud@gmail.com' 
        recipient_email = email
        subject = 'ğŸ“Œ [ì²œì¬ITêµìœ¡ì„¼í„°] ì´ë²ˆì£¼ ë‚˜ì—ê²Œ ë§ëŠ” ì±„ìš©ê³µê³ ëŠ”?' 
        feedback_ = 'ğŸ“Œ [ì²œì¬ITêµìœ¡ì„¼í„°] ì±„ìš©ê³µê³  í”¼ë“œë°± ìš”ì²­' 

        send_email(sender_email, recipient_email, subject, html_res) 

# ê°€ì ¸ì˜¨ ë°ì´í„°ë¥¼ unique_data_listë¡œ ì €ì¥
db_connection = connect_to_lms_database()
unique_data_list = get_unsub_data(db_connection)

# ë©”ì¸ ì²˜ë¦¬ ë¡œì§
post_json_path = '/home/ubuntu/job_posting/send_posting_log.json'
unique_jobs, grouped_data = fetch_and_deduplicate_data(course_list)
send_emails(unique_jobs, unique_data_list, post_json_path)
